It is a master worker node architecure.

Master contains API server, Controllar Manager, Schedular and ETCD.
Pods will run on the worker nodes.

Basic Commands for POD:
kubectl get pods -n default -o wide      
kubectl create -f pod.yaml
k run nameOfPod --image:image_name -n namespace
kubectl apply -f pod.yaml
kubectl describe pod pod-name
kubectl logs pod-name -c container-name
kubectl delete pod webapp
kubectl get pods --field-selector=status.phase=Running
k replace --force -f pod.yaml
k get pods[or all] --selector=env=dev --no-headers | wc -l // --selector=env=prod,bu=finance for and operations [or --selector=env=prod --selector=bu=finance for or operations


Basic Commands for rs:
kubectl get rs
kubectl get replicasets
kubectl describe rs rs-name
kubectl scale --replicas=5 rs rs-name
kubectl edit rs rs-name

Basic Commands for Deployment:
kubectl get deployments
k edit deployment deployment_name

Deployment VS ReplicaSets vs Replication Controller
RC- older version mostly immperative and now it is depricated
RS- Good for auto provising the Pods
DP- advanced RS supports rollbacks and rollouts properly.

Basic Commands for Service:
k get svc
k get endpoints service_name

basic commands for namespace:

k create namespace namespace_name
k get pods -A(or --all-namespaces)
k get pod -n=marketing (or -n marketing)

imperrative commands:

k run pod-name --image=image_name --namespace=dev --port=8080 --labels=tier=db
k expose pod pod-name --type=clusterIP --port=8080
k create deployment deployment_name --image=image_name --replicas=2 
k create svc clusterIP nameOfService --tcp=4842:8428

Labels and Selectors:
k describe node node01   // check lables
k label node node01 color=blue

correct way to access the service in other namespace is db-service.dev.svc.cluster.local

creating Pods:

apiVersion: v1
kind: Pod

metadata:
  name: mypod
  labels:
    app: frontend

spec:
  nodeName: node01
  containers:
  - name: mypod
    image: nginx
  tolerations:
    - key: "taintKey"
      value: "taintValue"
      operator: "Equal"
      effect: "NoScheduling"

  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:    // ignoredDuringSchedulingIgnoredDuringExecution
        nodeSelectorTerms:
        - matchExpressions:
          - key: keyname
            operator: In    // In, NotIn or Exists
            values:
            - value1
            - value2 // if needed.

  prorityClassName: high-pc

creating RS:

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: rs-name
  labels:
    key: value
spec:
  replicas: 2
  selector:
    matchLabels:
      podname: pod1
  template:
    metadata:
      labels:
        podname: pod1
    spec:
      containers:
        - name: containername
          image: nginx



creating Deployment:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment1
  labels:
    tier: frontend
spec:
  replicas: 2
  selector:
    matchLabels:
      apps: t
  template:
    metadata:
      labels:
        apps: t
    spec:
      containers:
      - name: container-name
        image: nginx

creating Services:
NodePort:

apiVersion: v1
kind: Service
metadata:
  name: s1
  labels:
    app: type
spec:
  type: NodePort
  selector: 
    key: value_of_deployment
  ports:
    - port: 8080
      targetPort: 8080
      nodePort: 30435 (30000<=NodePort<=32000)

ClusterIP and LoadBalancer:

apiVersion: v1
kind: Service
metedata:
  name: s2
spec:
  type: ClusterIP or LoadBalancer
  selector:
    name: name0fpods
  ports:
    - port: 8080
      targetPort: 8080

ExternalName:

apiVersion: v1
kind: Service
metadata:
  name: s4
spec:
  type: ExternalName
  externalName: example.com


Creating Namespace:

apiVersion: v1
kind: Namespace
metadata:
  name: devhari



Taints: It may keep p1 in n1 or it can keep p1 in any other node
To set taint: k taint nodes node01 taintKey=taintValue:[NoScheduling/preferNoScheduling/NoExecute]
To Remove taint: k taint node node01 taintKey=taintValue:NoScheduling-  // hint: simply add - at end to remove taint

NoScheduling-> Doesn't schedule if no tolerations set on POD
preferNoScheduling-> Might schedule the pod if no tolerations set and no other nodes are available
NoExecute-> make force evacivates if no tolarations set on pod.

nodeSelector: It can keep p1 in only n1 but if you say keep p1 in n1 or n2 it can't

NodeAffinity: It can keep p1 in n1 or n2.

Daemonsets: It ensure daemonset pod run in every node even if you add nodes. ex: kube-proxy. good for monitering and logging.
It is same as replicaset but kind: Daemonset and it doesn't have any replicas.
k get daemonsets
k describe daemonsets name

Static Pods: without api-server kubelet can create pods. It reads /etc/kubernetes/manifests folder and we can't create RS or deployments. used to deploy control plane componets.
in /var/lib/kubelet/config.yaml file you can see --pod-manifest-path=folder_path_to_static_pods

Priority Classes: to make priorities while scheduling. it can be in between -2Billion tp 1billion. but 1-2billions is used by k componets.

k get priorityclass/pc
kubectl get pods -o custom-columns="NAME:.metadata.name,PRIORITY:.spec.priorityClassName"

apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: hig-pc
value: 100000
globalDefault: false

